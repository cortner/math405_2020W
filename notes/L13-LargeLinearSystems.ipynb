{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Numerical Methods for Differential Equations\n",
    "\n",
    "[[Instructor: Christoph Ortner]](http://www.math.ubc.ca/~ortner/)  [[CANVAS]](https://canvas.ubc.ca/courses/55324)\n",
    "\n",
    "\n",
    "## Solving Large Linear Systems\n",
    "\n",
    "This is an ultra-rapid introduction to a topic that I've neglected throughout this course, but is extremely important once you start solving \"real-world problems\". The idea is to give you some initial exposure and intuition on a few important ideas\n",
    "\n",
    "* complexity of direct solvers for 2D, 3D systems\n",
    "* the idea of iterative solvers (steepest descent for linear systems)\n",
    "* Krylov subspace methods (Conjugate gradient method)\n",
    "* Cascading multigrid (towards linear scaling!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Literature\n",
    "\n",
    "There are many good lecture notes, and slides on these topics. Here are some that I used to prepare for this lecture:\n",
    "\n",
    "* [[pdf]](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) An Introduction to the Conjugate Gradient Method Without the Agonizing Pain by J. Shewchuck, \n",
    "* [[pdf]](http://www.maths.lth.se/na/courses/NUM115/NUM115-05/krylov.pdf) The Idea Behind Krylov Methods, by Ilse C.F. Ipsen and Carl D. Meyer\n",
    "\n",
    "Cascadic Multigrid: \n",
    "* [[html]](https://link.springer.com/article/10.1007/s002110050234) The cascadic multigrid method for elliptic problems, Folkmar A. Bornemann & Peter Deuflhard , Numer. Math. 75, pages135–152(1996)\n",
    "* [[pdf]](https://opus4.kobv.de/opus4-zib/files/119/SC-93-23.pdf) Deuflhard, P. Cascadic conjugate gradient methods for elliptic partial differential equations. Algorithm and numerical results.\n",
    "\n",
    "\"Proper\" Multi-grid:\n",
    "* [[pdf]](https://www.math.ust.hk/~mawang/teaching/math532/mgtut.pdf) A Multigrid Tutorial, by William L. Briggs\n",
    "* [[pdf]](https://people.cs.kuleuven.be/~karl.meerbergen/didactiek/h03g1a/multigrid.pdf) A short multigrid tutorial, by Karl Meerbergen\n",
    "* [[pdf]](http://helper.ipam.ucla.edu/publications/es2002/es2002_1449.pdf) Multigrid (MG) and Local Refinement for Elliptic Partial Differential Equations, by Klaus Stüben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "include(\"../math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Direct Solvers for 2D, 3D Elliptic Problems\n",
    "\n",
    "This is a very brief review of [WS03](notes/WS03-FDiff2D.ipynb)\n",
    "\n",
    "We will restrict attention to the $d$-dimensional Laplace equation, \n",
    "$$\\begin{aligned}\n",
    "    -\\Delta u = f, \\qquad x \\in \\Omega, \\\\ \n",
    "        u = 0, & x \\in \\partial \\Omega,\n",
    "\\end{aligned}$$\n",
    "where $\\Omega = (0, 1)^d$ and $\\partial\\Omega$ denotes its boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Discretisation: 1D grid $x_n = n/N, n = 0, \\dots, N$, then\n",
    "$$\n",
    "    U_{\\bf n} = U_{n_1 \\cdots n_d} \\approx u(x_{n_1}, \\dots, x_{n_d})\n",
    "$$\n",
    "\n",
    "$$\n",
    "    - \\Delta_h U = F,\n",
    "$$\n",
    "where $F_{\\bf n} = f(x_{n_1}, \\dots, x_{n_d})$ and \n",
    "$$\n",
    "    \\Delta_hU_{{\\bf n}} = \\sum_{i = 1}^d \\frac{U_{{\\bf n} + {\\bf e}_i} - 2 U_{{\\bf n}} + U_{{\\bf n} - {\\bf e}_i}}{h^2}\n",
    "    = \\Big( D_h^2 \\otimes I \\otimes \\cdots \\otimes I\n",
    "            + I \\otimes D_h^2 \\otimes I \\otimes \\cdots \\otimes I\n",
    "            + \\cdots\n",
    "            + I \\otimes \\cdots \\otimes I \\otimes D_h^2 \\Big) U_{{\\bf n}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The cost vs error doesn't look good: \n",
    "\n",
    "\n",
    "|dimension | error     | system size | cost - order  |\n",
    "|----------|-----------|-------------|---------------|\n",
    "|  1       |  N^-2     | N   x N     |  N            |\n",
    "|  2       |  N^-2     | N^2 x N^2   |  N^3          |\n",
    "|  3       |  N^-2     | N^3 x N^3   |  N^6          |\n",
    "\n",
    "Because sparse solvers are very highly optimized, 2D tends to be just fine, while 3D is often no longer tractable for \"real-world\" problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this implementation uses dirichlet boundary conditions only.\n",
    "L1d(N) = sparse(Tridiagonal(-N^2 * ones(N-2), 2*N^2*ones(N-1), -N^2*ones(N-2)))\n",
    "Id(N) = sparse(I, (N-1,N-1))\n",
    "L2d(N) = sparse(kron(L1d(N), Id(N)) + kron(Id(N), L1d(N)))\n",
    "L3d(N) = sparse(kron(L1d(N), Id(N), Id(N)) + kron(Id(N), L1d(N), Id(N)) + kron(Id(N), Id(N), L1d(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "NN_timings = (2).^(2:9)\n",
    "timings_direct = zeros(3, length(NN_timings))\n",
    "for (iN, N) in enumerate(NN_timings), (d, Lfun) in enumerate((L1d, L2d, L3d))\n",
    "    if N > 2^6 && d == 3; timings_direct[d, iN] = NaN; continue; end \n",
    "    timings_direct[d, iN] = (@elapsed (Lfun(N) \\ ones((N-1)^d)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "P = plot(; size = (400, 300), yscale = :log10, xscale = :log10, legend = :outertopright, ylims = (1e-5, 1e1)) \n",
    "C = [3e-6, 1e-7, 1e-9]; pred = [1, 3, 6]; NN = NN_timings[2:end]\n",
    "for d = 1:3\n",
    "    plot!(NN, timings_direct[d, 2:end], c = d, label = \"d = $d\", lw=3, ms=6, m=:o)\n",
    "    plot!(NN, C[d] * NN.^pred[d], c=d, ls=:dash, label = \"\", lw=2)\n",
    "end\n",
    "plot!(NN, 1e-6 * NN.^2, c=2, ls=:dot, label = \"\", lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative Solvers : Motivation\n",
    "\n",
    "For the laplace equation, the finite difference system matrix $A$ is symmetric, positive definite (SPD). In this case, solving $Ax = b$ is equivalent to\n",
    "$$\n",
    "    x = \\arg\\min \\Phi(x) := \\frac{1}{2} x^T A x - x^T b.\n",
    "$$\n",
    "**Proof:** Easy exercise, show that $\\nabla \\Phi(x) = Ax - b$\n",
    "\n",
    "**IDEA:** Use steepest descent to solve this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\nabla \\Phi(x) = Ax - b$, hence \n",
    "$$\n",
    "    x_{n+1} = x_n - \\alpha_n (A x_n - b)\n",
    "$$\n",
    "Minimize $\\Phi(x_{n+1})$ with respect to $\\alpha_n$, ..., a bit of algebra, ..., \n",
    "$$ \n",
    "    x_{n+1} = x_n - \\frac{g_n^T g_n}{g_n^T A g_n} g_n,  \\qquad g_n := A x_n - b.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**KEY OBSERVATIONS:**\n",
    "* We only need $A x$ but not $A$ itself!\n",
    "* No storage required and cost of $A x$ is $O(N^d)$ for all $d$!\n",
    "* Convergence: can prove (nontrivial!)\n",
    "$$\n",
    "    \\| x_n - x \\| \\leq \\kappa \\Big( \\frac{\\kappa - 1}{\\kappa + 1} \\Big)^n \\|x_0 - x\\|\n",
    "$$\n",
    "where $\\kappa = {\\rm cond}(A)$. \n",
    "* For the Laplace equation: $\\kappa \\approx N^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"2D discrete laplace operator in matrix format\"\n",
    "function laplace_mat(V)  \n",
    "    N = size(V, 1)-1\n",
    "    AxV = zeros(N+1, N+1)\n",
    "    for n1 = 2:N, n2 = 2:N\n",
    "        AxV[n1, n2] = N^2 * (- V[n1+1,n2] - V[n1-1,n2] - V[n1,n2+1] - V[n1,n2-1] + 4 * V[n1,n2])\n",
    "    end\n",
    "    return AxV \n",
    "end\n",
    "\n",
    "\"Apply `nsteps` steepest descent iterations\"\n",
    "function steepest_descent(R, nsteps)\n",
    "    W = zeros(size(R))\n",
    "    for n = 1:nsteps\n",
    "        G = laplace_mat(W) - R \n",
    "        W -= norm(G)^2 / dot(G, laplace_mat(G)) * G\n",
    "    end\n",
    "    return W \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 32\n",
    "A = L2d(N) \n",
    "kappa = cond(Matrix(A)); @show kappa \n",
    "u = A \\ ones((N-1)^2)\n",
    "F = zeros(N+1, N+1); F[2:N, 2:N] .= 1\n",
    "NSTEPS = 20:20:400\n",
    "Err = [ norm(steepest_descent(F, nsteps)[2:N,2:N][:]-u, Inf) for nsteps in NSTEPS ]\n",
    "plot(NSTEPS, Err, lw=3, label = \"||x_n - x||\", yscale = :log10, m=:o, ms=5, size = (400, 300), title = \"N = $N\")\n",
    "q = (kappa - 1) / (kappa + 1)\n",
    "plot!(NSTEPS[10:end], 0.08 * q.^NSTEPS[10:end], c=:black, lw=2, ls=:dash, label = \"predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Krylov Subspace Methods\n",
    "\n",
    "There are many beautiful and insightful ways to motivate and introduce Krylov subspace methods ... all of them take quite a lot of legwork. Since we don't have that time here, I'll be extremely \"sparse\". Maybe somebody wants to pickup this topic for a presentation?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Idea:** starting from gradient descent, \n",
    "$$\n",
    "    x_{n+1} = x_n - \\alpha_n \\nabla \\Phi(x_n) =  x_n - \\alpha_n (A x_n - b)\n",
    "$$\n",
    "\n",
    "This means that $x_{n+1}$ must be a linear combinatin of $b, A x_0, \\dots, A x_n$ and by induction, if we start with $x_0 = 0$, then it must be a linear combination of \n",
    "$$\n",
    "    b, A b, A^2 b, \\dots, A^n b\n",
    "$$\n",
    "Let's just remember these vectors, and form a *Krylov subspace* \n",
    "$$\n",
    "    K_n := {\\rm span}\\big\\{ b, Ab, A^2 b, \\dots, A^n b \\big\\}.\n",
    "$$\n",
    "and then choose the \"optimal\" approximate solution from this subspace:\n",
    "$$\n",
    "    x_n^{\\rm cg} := \\arg\\min_{K_n} \\Phi(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Conjugate Gradient Method\n",
    "\n",
    "**Non-trivial Observation 1** : $x_n := x_n^{\\rm cg}$ satisfies a 3-term recurrance. I.e., we do not need to remember $K_n$. By enlarging the state space from $x_n$ to $(x_n, r_n, d_n)$ we can write it as a convenient 2-term recurrance:  \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    d_0 &= b - A x_0 \\\\ \n",
    "    x_{n+1} &= x_n + \\alpha_n d_n \\\\ \n",
    "    r_{n+1} &= r_n - \\alpha_n A d_n \\\\ \n",
    "    d_{n+1} &= r_{n+1} + \\beta_{n+1} d_n \\\\ \n",
    "\\end{aligned}$$\n",
    "where \n",
    "$$\\begin{aligned}\n",
    "    \\alpha_{n+1} &= \\frac{r_n^T r_n}{d_n^T A d_n}, \\\\ \n",
    "    \\beta_{n+1} &= \\frac{r_{n+1}^T r_{n+1}}{r_n^T r_n}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Proof:** non-trivial, requires a lot of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Non-trivial Observation 2:** \n",
    "$$\n",
    "    \\| x_n - x \\| \\leq \\kappa \\bigg( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa + 1}}\\bigg)^n \\| x_0 - x \\|.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Cost Comparison:** If $\\epsilon_n = \\|x_n - x\\|$ (for CG or SD) then \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    n &\\approx \\kappa |\\log \\epsilon_n|, \\qquad \\text{SD} \\\\ \n",
    "    n &\\approx \\sqrt{\\kappa} |\\log \\epsilon_n|, \\qquad \\text{CG}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For the Laplace operator: $\\kappa \\approx N^2$, i.e. \n",
    "$$\\begin{aligned}\n",
    "    n &\\approx N^2 |\\log \\epsilon_n|, \\qquad \\text{SD} \\\\ \n",
    "    n &\\approx N |\\log \\epsilon_n|, \\qquad \\text{CG}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<!-- Cost of one iteration is $O(N^d)$, discretisation error $\\epsilon$ is $N^{-2}$ hence\n",
    "$$\\begin{aligned}\n",
    "    {\\rm Cost}  &\\approx \\epsilon^{-1-d/2} |\\log\\epsilon|, \\qquad \\text{SD}, \\\\ \n",
    "    {\\rm Cost}  &\\approx \\epsilon^{-1/2-d/2} |\\log\\epsilon|, \\qquad \\text{CG}.\n",
    "\\end{aligned} $$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using IterativeSolvers, LinearMaps\n",
    "\n",
    "# Julia's built-in iterative solvers require us to use (abstract) matrices and vectors\n",
    "# for simplicity we therefore convert between matrix and vector formulation: \n",
    "\n",
    "mat2vec(V) = V[2:end-1, 2:end-1][:]\n",
    "\n",
    "function vec2mat(v)\n",
    "    N = round(Int, sqrt(length(v))) + 1\n",
    "    V = zeros(N+1, N+1)\n",
    "    V[2:N, 2:N] .= reshape(v, N-1, N-1)\n",
    "    return V\n",
    "end\n",
    "\n",
    "laplace_vec = mat2vec ∘ laplace_mat ∘ vec2mat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "L = LinearMap(laplace_vec, (N-1)^2; issymmetric=true, ismutating=false)\n",
    "F = mat2vec(ones(N+1, N+1))\n",
    "X = vec2mat( cg(L, F) )\n",
    "plot(surface(X), contourf(X), size = (600, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Convergence test \n",
    "N = 128\n",
    "U = vec2mat(L2d(N)  \\ ones((N-1)^2))\n",
    "L = LinearMap(laplace_vec, (N-1)^2; issymmetric=true, ismutating=false)\n",
    "F = zeros(N+1, N+1); F[2:N,2:N] .= 1; b = mat2vec(ones(N+1, N+1))\n",
    "NSTEPS = 20:20:200\n",
    "ErrSD = [ norm(steepest_descent(F, nsteps)-U, Inf) for nsteps in NSTEPS ]\n",
    "ErrCG = [ norm(vec2mat( cg(L, b; maxiter = nsteps) ) - U, Inf) for nsteps in NSTEPS ]\n",
    "\n",
    "plot(NSTEPS, ErrSD, lw=3, label = \"SD\", yscale = :log10, m=:o, ms=5, size = (400, 300), title = \"N = $N\")\n",
    "plot!(NSTEPS, ErrCG, lw=3, label = \"CG\", m=:o, ms=:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But despite the excellent performance of CG, it still deteriorates rapidly with increasing system size! We end up with the same scaling as a direct solver! (but much lower memory requirement!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NN = (2).^(3:8) \n",
    "num_cg_iters = _N -> cg(LinearMap(laplace_vec, (_N-1)^2; issymmetric=true, ismutating=false), \n",
    "                        mat2vec(ones(_N+1, _N+1)), log = true)[2].iters\n",
    "plot(NN, num_cg_iters.(NN), lw = 3, m = :o, ms=7, label = \"# iterations\", \n",
    "     ylabel = \"N\", yscale = :log10, xscale = :log10, legend = :topleft, size = (400, 250))\n",
    "plot!(NN[3:end], NN[3:end], c=:black, label = L\"\\sim N \\propto \\sqrt{\\kappa}\", s=:dash, lw=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's add CG to our timings test!\n",
    "solve_cg2 = N -> cg(LinearMap(laplace_vec, (N-1)^2; issymmetric=true, ismutating=false), \n",
    "                   mat2vec(ones(N+1, N+1)))\n",
    "solve_cg3 = N -> cg(L3d(N), ones((N-1)^3)) # (don't have a matrix-free version implemented, sorry)\n",
    "timings_cg2 = [ (@elapsed solve_cg2(N)) for N in NN_timings ]\n",
    "timings_cg3 = [[ (@elapsed solve_cg3(N)) for N in NN_timings[1:end-2] ]; [NaN,NaN] ]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "P = plot(; size = (400, 300), yscale = :log10, xscale = :log10, legend = :outertopright, ylims = (7e-5, 3e1)) \n",
    "for d = 2:3\n",
    "    plot!(NN_timings[2:end], timings_direct[d, 2:end], c = d, label = \"direct $(d)D\", lw=3, ms=6, m=:o)\n",
    "end\n",
    "plot!(NN_timings[2:end], timings_cg2[2:end], c=4, label = \"CG 2D\", lw=3, ms=6, m=:o)\n",
    "plot!(NN_timings[2:end], timings_cg3[2:end], c=5, label = \"CG 3D\", lw=3, ms=6, m=:o)\n",
    "plot!(NN_timings[2:end], 5e-8 * NN_timings[2:end].^3, c=:black, ls=:dash, label = \"\", lw=2)\n",
    "plot!(NN_timings[2:end-2], 5e-8 * NN_timings[2:end-2].^4, c=:black, ls=:dash, label = \"\", lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Scaling of direct vs CG: \n",
    "\n",
    "|  dimension | direct    |  CG      |\n",
    "|------------|-----------|----------|\n",
    "|    1       | N         |  N^2 log ϵ     |\n",
    "|    2       | N^3 (N^2) |  N^3 log ϵ     |\n",
    "|    3       | N^6       |  N^4 log ϵ     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Linear Scaling!\n",
    "\n",
    "**Motivation: Linear Scaling Algorithms** \n",
    "* Thomas algorithm for 1D FD problems: cost = $O(N)$\n",
    "* FFT for fourier spectral methods: cost = $O(N^d \\log N)$ in all dimensions.\n",
    "\n",
    "Linear scaling or close to linear scaling is one of the main goals of scientific computing!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Cascadic Multigrid Method\n",
    "\n",
    "![](figs/cascadic_mg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cascadic Multi-grid in Pseudocode:\n",
    "```\n",
    "h = h0\n",
    "U[h] = solve(h)\n",
    "while h > h_fine\n",
    "    Uc = U[h]          # remember the solution from the previous grid\n",
    "    h <- h/2           # refine the grid\n",
    "    Ui = prolong(Uc)   # \"prolong\" Uc onto the finer grid\n",
    "    U[h] = cg( init = Ui )    # solve in fine grid with good initial guess\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"prolong a coarse-grid function to a fine-grid function\"\n",
    "function mg_prolong(V)\n",
    "    N = size(V, 1) - 1\n",
    "    Nf = 2 * N\n",
    "    Vf = zeros(Nf+1, Nf+1)\n",
    "    for n1 = 2:N, n2 = 2:N\n",
    "        n1f = 2 * (n1-1) + 1; n2f = 2 *(n2-1) + 1\n",
    "        Vf[n1f, n2f] = V[n1, n2]\n",
    "        Vf[n1f+1, n2f] += 0.5 * V[n1, n2]\n",
    "        Vf[n1f-1, n2f] += 0.5 * V[n1, n2]\n",
    "        Vf[n1f, n2f+1] += 0.5 * V[n1, n2]\n",
    "        Vf[n1f, n2f-1] += 0.5 * V[n1, n2]\n",
    "        Vf[n1f+1, n2f+1] += 0.25 * V[n1, n2]\n",
    "        Vf[n1f+1, n2f-1] += 0.25 * V[n1, n2]\n",
    "        Vf[n1f-1, n2f+1] += 0.25 * V[n1, n2]\n",
    "        Vf[n1f-1, n2f-1] += 0.25 * V[n1, n2]\n",
    "    end\n",
    "    return Vf \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cg_iters(U0, nsteps) = (N = size(U0, 1)-1; \n",
    "                        cg!(mat2vec(U0), LinearMap(laplace_vec, (N-1)^2; issymmetric=true, ismutating=false), \n",
    "                            mat2vec(ones(N+1, N+1)), maxiter = nsteps) |> vec2mat)\n",
    "function cascadic_mg(levels, niter; N = 2)\n",
    "    U = zeros(N÷2+1,N÷2+1)\n",
    "    for l = 1:levels\n",
    "        U = mg_prolong(U)\n",
    "        U = cg_iters(U, niter)\n",
    "    end\n",
    "    return U\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "U = cascadic_mg(4, 10)\n",
    "plot(surface(U), contourf(U), size = (600, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number of levels determines $h$ i.e. the accuracy of the \n",
    "finite difference scheme. The only \"tuning\" parameter we have \n",
    "available to tweak the accuracy of the linear solver is \n",
    "`niter` the number of CG iterations at each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Convergence Test \n",
    "NITER = [5, 10, 20, 40, 1_000]\n",
    "LEVELS = collect(2:6)\n",
    "ERRs = zeros(length(LEVELS), length(NITER))\n",
    "for (il, levels) in enumerate(LEVELS)\n",
    "    U = cascadic_mg(levels, 2)\n",
    "    N = size(U, 1) - 1\n",
    "    Ux = vec2mat(L2d(N) \\ ones((N-1)^2))  # direct FD solution \n",
    "    for (iit, niter) in enumerate(NITER)\n",
    "        Umg = cascadic_mg(levels, niter)  # cascaded multigrid solution\n",
    "        ERRs[il, iit] = norm(Ux - Umg, Inf)\n",
    "    end    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "header = [ [\"#lev\"]; [\"#iter = $(NITER[1])\"]; string.(NITER[2:end-1]); [\"Inf\"] ]\n",
    "data = [LEVELS ERRs]\n",
    "pretty_table(data, header; \n",
    "             formatters = (v,i,j) -> (j == 1 ? Int(v) : (@sprintf(\"%.2e\", v))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cascadic Multigrid - Cost Analysis\n",
    "\n",
    "* Cost per iteration: $O(N_l^d)$ x #iterations; hence overall cost is \n",
    "$$\n",
    "   {\\rm COST} \\approx \\text{\\# iter} \\times \\sum_l N_l^d \\approx \\text{\\# iter} \\times N_{\\rm fine}^d.\n",
    "$$\n",
    "* Our convergence analysis suggests (and one can prove this!) the number of required iterations scales as $\\log \\epsilon$ (desired accuracy)\n",
    "* We want $\\epsilon = N^{-2}$ which suggests choosing $\\# iter = C \\log N^2$. \n",
    "* Overall cost:\n",
    "$$\n",
    "    {\\rm COST}(N) \\approx N^2 \\log N\n",
    "$$\n",
    "\n",
    "**Cascadic MG is almost a linear scaling method!**\n",
    "\n",
    "Our implementation is a bit naive and needs to be tweaked in a variety of ways, but the basic idea is a good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming that claim ..\n",
    "NN = (2).^(2:12)\n",
    "solve_cmg = N -> cascadic_mg(round(Int, log2(N)), round(Int, 5 * log10(N)))\n",
    "tim_direct = [ (L = L2d(N); b = ones((N-1)^2); @elapsed (L \\ b)) for N in NN ]\n",
    "tim_cg2 = [[ (@elapsed solve_cg2(N)) for N in NN[1:end-2] ]; [NaN, NaN]]\n",
    "tim_cmg = [ (@elapsed solve_cmg(N)) for N in NN ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(; size = (400, 300), yscale = :log10, xscale = :log10, legend = :outertopright, ylims = (7e-5, 3e2)) \n",
    "plot!(NN[2:end], tim_direct[2:end], c = 1, label = \"direct 2D\", lw=3, ms=6, m=:o)\n",
    "plot!(NN[2:end], tim_cg2[2:end], c=2, label = \"CG 2D\", lw=3, ms=6, m=:o)\n",
    "plot!(NN[2:end], tim_cmg[2:end], c=3, label = \"C-MG 2D\", lw=3, ms=6, m=:o)\n",
    "plot!(NN[2:end], 1e-8 * NN[2:end].^3, c=:black, ls=:dash, label = \"\", lw=2)\n",
    "plot!(NN[2:end], 1e-6 * NN[2:end].^2, c=:black, ls=:dash, label = \"\", lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A deeper (slightly less superficial) look at why C-MG works well.\n",
    "\n",
    "**Basic message:** \n",
    "* When prolonging $U[h] \\to U[h/2]$, the remaining error is a high-frequency error. \n",
    "* High frequency errors are the first that get dampened by typical iterative solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plots = []\n",
    "plargs = (lw=0.2, colorbar=false, xticks = [], yticks = [])\n",
    "for N in [4, 8, 16]\n",
    "    Uc = vec2mat( L2d(N) \\ ones((N-1)^2) )\n",
    "    Ui = mg_prolong(Uc)\n",
    "    Uf = vec2mat( L2d(2*N) \\ ones((2*N-1)^2) )\n",
    "    append!(plots, [contourf(Uc; plargs...), contourf(Uf; plargs...), contourf(Uf - Ui; plargs..., lw=0)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(plots..., size = (600, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions / Take-home Messages\n",
    "\n",
    "* Be aware of the scaling of computational cost and memory requirements for 2D and 3D problems!\n",
    "* Iterative solvers are a useful choice for 2D problems and a necessary choice for 3D problems!\n",
    "* Linear scaling is possible but often difficult to obtain and may require deep insight into the problem being solved.\n",
    "* Not all problems can be solved by multi-grid methods! Different problems require different ideas to achieve linear scaling complexity. \n",
    "* Multigrid is often more naturally interpreted as a preconditioner. See separate recorded lecture or workshop (TBD!) **Highly recommended for those of you interested in scientific computing!**"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
